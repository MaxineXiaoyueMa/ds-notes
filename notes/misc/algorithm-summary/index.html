



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-4.6.3">
    
    
      
        <title>Machine Learning Concept - XIAOYUE'S NOTES</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.adb8469c.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#ab47bc">
      
    
    
      <script src="../../../assets/javascripts/modernizr.86422ebf.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../../css/pandas-dataframe.css">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="purple" data-md-color-accent="pink">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#machine-learning-concept" tabindex="0" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../.." title="XIAOYUE'S NOTES" aria-label="XIAOYUE'S NOTES" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              XIAOYUE'S NOTES
            </span>
            <span class="md-header-nav__topic">
              
                Machine Learning Concept
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../.." title="XIAOYUE'S NOTES" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    XIAOYUE'S NOTES
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../" title="Notes" class="md-nav__link">
      Notes
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#algorithms" class="md-nav__link">
    Algorithms
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    Regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression-algorithms" class="md-nav__link">
    Regression Algorithms
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-metrics" class="md-nav__link">
    Regression metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    Classification
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-algorithms" class="md-nav__link">
    Classification Algorithms
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification-metrics" class="md-nav__link">
    Classification Metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unsupervised" class="md-nav__link">
    Unsupervised
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clustering" class="md-nav__link">
    Clustering
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#modeling" class="md-nav__link">
    Modeling
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-tuning" class="md-nav__link">
    Model tuning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recommender-system" class="md-nav__link">
    Recommender System
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#natural-language-processing-nlp" class="md-nav__link">
    Natural Language Processing (NLP)
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="machine-learning-concept">Machine Learning Concept<a class="headerlink" href="#machine-learning-concept" title="Permanent link">&para;</a></h1>
<h2 id="algorithms">Algorithms<a class="headerlink" href="#algorithms" title="Permanent link">&para;</a></h2>
<h3 id="regression">Regression<a class="headerlink" href="#regression" title="Permanent link">&para;</a></h3>
<h4 id="regression-algorithms">Regression Algorithms<a class="headerlink" href="#regression-algorithms" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>Linear Regression</strong>
  <strong>LASSO</strong>: regularization term is the L1 norm sum of absolute values of parameters
  <strong>Ridge</strong>: regularization term is the L2 norm - sum of squared values of parameters</li>
<li><strong>Decision Trees</strong></li>
<li><strong>Random Forest Regressor</strong>:</li>
<li><strong>Gradient Boosted Trees Regressor</strong></li>
<li><strong>Adaboost Regressor</strong>:</li>
<li><strong>Support Vector Regression</strong>: find the hyperplane such that all observations fall in the band. Support vectors are the vectors that are on the edge of the band.</li>
</ol>
<h4 id="regression-metrics">Regression metrics<a class="headerlink" href="#regression-metrics" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>MSE</strong>(Mean Squared Error): popular metric for regression models as a loss function, calculates the average of squared error = (y truth - y prediction). Note large error's are penalized much heavier than small errors. Parameters update much faster during graident descent with this loss function at large errors.</li>
<li><strong>RMSE</strong>(Root Mean Squared Error): square root of MSE, but has same unit as predition</li>
<li><strong>MAE</strong>(Mean Absolute Error): caculate the average of the absolute value of error. Note parameter updates the same speed across error range in contrast to MSE due to linear increase of error in loss.</li>
<li><strong>R-Squared</strong>(Coefficient of Determination): the proportion of variance in y that is explained by the features. 1 - SSE(Sum of Squared Errors)/TSS(Total Sum of Squares). Default metric for regression models on sklearn.</li>
<li><strong>Adjusted R-Squared</strong>(Adjusted Coefficient of Determination): like R-Squared, but penalizes the amount of features used in prediction, is always smaller than R-Squared. 1 - (SSE/(n-k))/(TSS/(n-1))</li>
</ol>
<h3 id="classification">Classification<a class="headerlink" href="#classification" title="Permanent link">&para;</a></h3>
<h4 id="classification-algorithms">Classification Algorithms<a class="headerlink" href="#classification-algorithms" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>Naive Bayes Classifier</strong> - generative model P(x, y)<blockquote>
<p>Essentially, comparing probability of having feature set X conditional on target being positive vs. negative.</p>
</blockquote>
</li>
</ol>
<p>P(y|X) = P(y&amp;X) / P(X)
P(1-y|X) = P((1-y)&amp;X) / P(X)
Since the denomenator is the same, compare P(y&amp;X) and P((1-y)&amp;X)</p>
<p>Reference: http://cs229.stanford.edu/notes/cs229-notes2.pdf</p>
<ol>
<li><strong>Logistic Regression</strong></li>
<li><strong>Decision Trees</strong></li>
<li><strong>Split criteria</strong>:<ol>
<li><strong>Gini Impurity</strong>: the probability of a randomly selected sample is wrongly classified</li>
<li><strong>Entropy</strong>: the amount of information(surprise) enclosed</li>
</ol>
</li>
<li><strong>Feature importance</strong>:<ol>
<li><strong>Gini importance</strong>: The average of impurity reduced by each feature</li>
<li><strong>Permutation importance</strong>: mean accuracy (or any other metrics) reduction due to permuting each features, averaged over all trees.</li>
</ol>
</li>
<li><strong>Random Forest Classifier</strong> - discriminative model P(y|x): Simultaneousely build a forest of trees that each independently make predictions on samples. Subsampling and using only a small group of features add randomness. Each tree can also be limited to control overfitting.</li>
<li><strong>Gradient Boosted Trees</strong></li>
<li><strong>Adaboost</strong>: Buildling estimators sequentially to lear better from mistakes. Adaptive by giving more weight to samples that were wrong.<ol>
<li>initiliaze equal weight 1/# of observations to each observation</li>
<li>iterate:<ol>
<li>calculate weighted error</li>
<li>calculate model weight with weighted error</li>
<li>update sample weight with weighted error</li>
<li>build another model with newly weighted samples</li>
</ol>
</li>
<li>calculate final prediction with prediction from each model weighted by their weights:<ol>
<li>weighted probability</li>
<li>majority vote</li>
</ol>
</li>
</ol>
</li>
<li><strong>Support Vector Machine</strong></li>
</ol>
<h4 id="classification-metrics">Classification Metrics<a class="headerlink" href="#classification-metrics" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>ROC Curve</strong>: rank observation from the most likely to least likely, a good separater will have have score
2.<strong>Accuracy</strong>: default metric for classification models, measure how many observations are correctly classified with default threshold 0.5, not a good measure for imbalanced data, or threshold other than 0.5.</li>
<li><strong>Precision-Recall Curve</strong>:</li>
</ol>
<h3 id="unsupervised">Unsupervised<a class="headerlink" href="#unsupervised" title="Permanent link">&para;</a></h3>
<h4 id="clustering">Clustering<a class="headerlink" href="#clustering" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>KMeans</strong>: find predetermined number of clusters. It is achieved by first pick cluster centers, then iteratively calculate euclidean distance between observations and cluter centers, assign observations to cluster centers, and recalculate cluster centers by averaging the distances of observations to corresponding cluster centers until convergence.</li>
<li><strong>Cluster Measures</strong>:<ol>
<li>cluster similarities: adjusted Rand Index (pairs that two clusering agress over total pairs adjusted for randomness)</li>
<li>how good the clusters are: silhouette score (using mean distance from each point to other points within the same cluster and mean distance from each point to points outside the same clusters. Good clustering result in small distance within cluster, and large distance outside cluster, final measure is close to 1.)</li>
</ol>
</li>
</ol>
<p>Reference:
- https://www.datarevenue.com/en-blog/building-a-city-recommender-for-nomads
- https://towardsdatascience.com/beginners-recommendation-systems-with-python-ee1b08d2efb6
- https://medium.com/data-science-101/movie-recommendation-system-content-filtering-7ba425ca0920
- https://towardsdatascience.com/how-did-we-build-book-recommender-systems-in-an-hour-the-fundamentals-dfee054f978e
- https://www.kaggle.com/gspmoreira/recommender-systems-in-python-101
- https://towardsdatascience.com/building-a-content-based-recommender-system-for-hotels-in-seattle-d724f0a32070</p>
<h2 id="modeling">Modeling<a class="headerlink" href="#modeling" title="Permanent link">&para;</a></h2>
<h3 id="model-tuning">Model tuning<a class="headerlink" href="#model-tuning" title="Permanent link">&para;</a></h3>
<ol>
<li>Random search</li>
<li>grid search</li>
<li>bayesian based optimization: "smartly" sample hyperparameter space by learning from previous trials. Gaussian process to poximate
Reference: https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture16.pdf
https://www.arxiv-vanity.com/papers/1012.2599/</li>
</ol>
<h2 id="recommender-system">Recommender System<a class="headerlink" href="#recommender-system" title="Permanent link">&para;</a></h2>
<p>Prediction of personal preference of items, match between user and product/item/experience/tactics/friend/mate.</p>
<ul>
<li>type:<ul>
<li><strong>random</strong>: random select, no "cold start",</li>
<li><strong>most popular</strong>: require some interaction data</li>
<li><strong>content based</strong>: robust to "cold start", recommendation based on similarities between items, i.e., "I like item A, B is similar to A, so, I probably like B", e.g., "if you bought this, you may also like these...", need one user interaction to start.</li>
<li><strong>collaborative filtering</strong>: recommendation based on similarities between personal tasts, e.g., he and I have similar tastes, so, if he likes it, then I will probably like it.<ul>
<li><strong>item based</strong>: if item A is liked by me, item A is also liked by him, and he likes item B, then, I will probably like B. e.g. "people who purchsed this also purchased these..."<ul>
<li><strong>vs. content based</strong>: item based CF uses implicit similarity between items with interaction/taste as medium, content based uses direct similarity between items</li>
</ul>
</li>
<li><strong>user based</strong>: e.g. "customers like you also likes"</li>
</ul>
</li>
</ul>
</li>
<li>similarity measures:<ul>
<li><strong>cosine similarity</strong>: dot product devided by the product of the norm of both vectors, linear kernel if vectors are already taken the norm in sklearn</li>
</ul>
</li>
<li>metric:<ul>
<li><strong>MAP@N</strong>: getting all N items recomded correctly scores 1 per AP@N. Getting more right per N is better, and getting right ealier is better. sum (Precision@k * change in Recall@k) for k in N.</li>
</ul>
</li>
</ul>
<p><strong>Reference</strong>:
- http://fastml.com/evaluating-recommender-systems/
- https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf</p>
<h2 id="natural-language-processing-nlp">Natural Language Processing (NLP)<a class="headerlink" href="#natural-language-processing-nlp" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Preprocessing</strong>:<ol>
<li><strong>Tokenization</strong>:</li>
<li><strong>Normalization</strong>:<ol>
<li>stemming: running -&gt; run</li>
<li>lemmatization: better -&gt; good
  3.</li>
</ol>
</li>
<li><strong>Substitution</strong>:</li>
</ol>
</li>
<li><strong>Feature Extraction</strong>: vectorization = text -&gt; numerical transformation<ol>
<li><strong>Bag of words</strong>: tokenizing, counting, (normalizing)</li>
<li><strong>tfidf</strong>: term weighting, term frequency (tf) times inverse document frequency (idf). idf is calculated as the log of number of documents devided by number of documents containing word, the more common the word, the less informational it is.<ol>
<li><strong>n_gram</strong>: order matters, 'like', and 'not like' mean opposite things, to preserve order, keep n_gram = 3</li>
<li><strong>stop_words</strong>: common words that don't add information: the, an, is, etc.</li>
</ol>
</li>
</ol>
</li>
</ol>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org" target="_blank" rel="noopener">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/MaxineXiaoyueMa/ds-notes" target="_blank" rel="noopener" title="github" class="md-footer-social__link fa fa-github"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.c33a9706.js"></script>
      
      <script>app.initialize({version:"1.1",url:{base:"../../.."}})</script>
      
    
  </body>
</html>